{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== [dissect_single_sample] ===\n",
      "[dissect_single_sample] => orig_atom_fea_len=92, atom_fea_len=64, nbr_fea_len=101, n_conv=3, h_fea_len=128, out_dim=1\n",
      "[dissect_single_sample] forward done. Output shape: torch.Size([1, 1])\n",
      "Model output value: 0.08809866011142731\n",
      "\n",
      "[dissect_single_sample] Intermediate outputs:\n",
      "  embedding: shape=(44, 64)\n",
      "  conv_0: shape=(44, 64)\n",
      "  conv_1: shape=(44, 64)\n",
      "  conv_2: shape=(44, 64)\n",
      "  fc_out: shape=(1, 1)\n",
      "[analyze_feature_activation] skip, arr.ndim=0\n",
      "\n",
      "=== [run_shap_analysis] ===\n",
      "[ModelWrapper] Inferred dims => orig_atom_fea_len=92, atom_fea_len=64, nbr_fea_len=101, n_conv=3, h_fea_len=128, out_dim=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Issues encountered while parsing CIF: Some fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "Using 200 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph embedding shape: torch.Size([6686, 128])\n",
      "y_test shape: (6686, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [04:39<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values shape: (200, 128)\n",
      "[SHAP] summary swarm plot saved -> shap_summary_swarm.png\n",
      "\n",
      "Top 10 important embedding dims:\n",
      "  GNN_Emb_73 = 0.0383\n",
      "  GNN_Emb_23 = 0.0326\n",
      "  GNN_Emb_118 = 0.0276\n",
      "  GNN_Emb_124 = 0.0221\n",
      "  GNN_Emb_106 = 0.0213\n",
      "  GNN_Emb_39 = 0.0206\n",
      "  GNN_Emb_56 = 0.0203\n",
      "  GNN_Emb_90 = 0.0197\n",
      "  GNN_Emb_61 = 0.0188\n",
      "  GNN_Emb_71 = 0.0186\n",
      "[SHAP] saved shap_values_emb.csv\n",
      "[SHAP] saved feature_importance_emb.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from cgcnn.data import CIFData, collate_pool\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# A. 自动解析模型维度的函数\n",
    "##############################################################################\n",
    "def infer_cgcnn_dims_from_sd(state_dict):\n",
    "    \"\"\"\n",
    "    根据 checkpoint 里的 key/shape，自动推断:\n",
    "     - orig_atom_fea_len, atom_fea_len\n",
    "     - nbr_fea_len, n_conv\n",
    "     - h_fea_len, out_dim\n",
    "    返回一个 (orig_atom_fea_len, atom_fea_len, nbr_fea_len, n_conv, h_fea_len, out_dim) 元组\n",
    "    \"\"\"\n",
    "    dims = {}\n",
    "\n",
    "    # 1) 解析 embedding.weight => 通常 shape [atom_fea_len, orig_atom_fea_len] (或反过来)\n",
    "    emb_key = \"embedding.weight\"\n",
    "    if emb_key not in state_dict:\n",
    "        raise ValueError(f\"Cannot find {emb_key} in checkpoint to infer dims.\")\n",
    "    emb_shape = state_dict[emb_key].shape  # e.g. (64, 92)\n",
    "    # 大多数 CGCNN 实现: \"out_features, in_features\" => out=atom_fea_len, in=orig_atom_fea_len\n",
    "    # 也有人写反，但这里假设 row=out_features\n",
    "    atom_fea_len, orig_atom_fea_len = emb_shape\n",
    "    dims[\"orig_atom_fea_len\"] = orig_atom_fea_len\n",
    "    dims[\"atom_fea_len\"] = atom_fea_len\n",
    "\n",
    "    # 2) 解析 convs.0.fc_full.weight => shape [2*atom_fea_len, 2*atom_fea_len + nbr_fea_len]\n",
    "    #    同时统计 n_conv\n",
    "    n_conv = 0\n",
    "    conv0_key = None\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"convs.\") and k.endswith(\".fc_full.weight\"):\n",
    "            n_conv += 1\n",
    "            # 只要第一层的形状来推断 nbr_fea_len\n",
    "            if conv0_key is None:\n",
    "                conv0_key = k\n",
    "\n",
    "    dims[\"n_conv\"] = n_conv\n",
    "    if conv0_key is None:\n",
    "        raise ValueError(\"Cannot find any convs.X.fc_full.weight in checkpoint.\")\n",
    "    conv0_shape = state_dict[conv0_key].shape  # e.g. (128, 229)\n",
    "    out_features, in_features = conv0_shape\n",
    "    # out_features = 2*atom_fea_len => double check\n",
    "    # in_features = 2*atom_fea_len + nbr_fea_len\n",
    "    # => nbr_fea_len = in_features - out_features\n",
    "    # out_features must match 2*dims[\"atom_fea_len\"]\n",
    "    if out_features != 2 * dims[\"atom_fea_len\"]:\n",
    "        raise ValueError(f\"fc_full out_features({out_features}) != 2*atom_fea_len({2*dims['atom_fea_len']}).\")\n",
    "    nbr_fea_len = in_features - out_features\n",
    "    dims[\"nbr_fea_len\"] = nbr_fea_len\n",
    "\n",
    "    # 3) 解析 fc_out.weight => shape (out_dim, h_fea_len)\n",
    "    fc_out_key = \"fc_out.weight\"\n",
    "    if fc_out_key not in state_dict:\n",
    "        raise ValueError(\"Cannot find fc_out.weight in checkpoint.\")\n",
    "    fc_out_shape = state_dict[fc_out_key].shape  # e.g. (1, 128)\n",
    "    out_dim, h_fea_len = fc_out_shape\n",
    "    dims[\"out_dim\"] = out_dim\n",
    "    dims[\"h_fea_len\"] = h_fea_len\n",
    "\n",
    "    return (\n",
    "        dims[\"orig_atom_fea_len\"],\n",
    "        dims[\"atom_fea_len\"],\n",
    "        dims[\"nbr_fea_len\"],\n",
    "        dims[\"n_conv\"],\n",
    "        dims[\"h_fea_len\"],\n",
    "        dims[\"out_dim\"]\n",
    "    )\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# B. ModifiedConvLayer\n",
    "##############################################################################\n",
    "class ModifiedConvLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    示范性的卷积层: (atom_fea_len, nbr_fea_len) -> fc_full -> BN -> split -> sum pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, atom_fea_len, nbr_fea_len):\n",
    "        super().__init__()\n",
    "        self.atom_fea_len = atom_fea_len\n",
    "        self.nbr_fea_len = nbr_fea_len\n",
    "\n",
    "        in_features = 2 * atom_fea_len + nbr_fea_len\n",
    "        out_features = 2 * atom_fea_len\n",
    "        self.fc_full = nn.Linear(in_features, out_features)\n",
    "        self.bn1 = nn.BatchNorm1d(out_features)\n",
    "        self.bn2 = nn.BatchNorm1d(atom_fea_len)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softplus1 = nn.Softplus()\n",
    "        self.softplus2 = nn.Softplus()\n",
    "\n",
    "    def forward(self, atom_in_fea, nbr_fea, nbr_fea_idx):\n",
    "        N, M = nbr_fea_idx.shape\n",
    "        atom_nbr_fea = atom_in_fea[nbr_fea_idx, :]  # (N, M, atom_fea_len)\n",
    "\n",
    "        total_nbr_fea = torch.cat([\n",
    "            atom_in_fea.unsqueeze(1).expand(N, M, self.atom_fea_len),\n",
    "            atom_nbr_fea,\n",
    "            nbr_fea\n",
    "        ], dim=2)  # shape => (N, M, 2*atom_fea_len + nbr_fea_len)\n",
    "\n",
    "        gated_fea = self.fc_full(total_nbr_fea.view(-1, total_nbr_fea.shape[-1]))\n",
    "        gated_fea = self.bn1(gated_fea).view(N, M, 2*self.atom_fea_len)\n",
    "\n",
    "        nbr_filter, nbr_core = gated_fea.chunk(2, dim=2)\n",
    "        nbr_filter = self.sigmoid(nbr_filter)\n",
    "        nbr_core = self.softplus1(nbr_core)\n",
    "\n",
    "        nbr_sumed = torch.sum(nbr_filter * nbr_core, dim=1)\n",
    "        nbr_sumed = self.bn2(nbr_sumed)\n",
    "        out = self.softplus2(atom_in_fea + nbr_sumed)\n",
    "        return out\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# C. CrystalGraphConvNetWithHooks\n",
    "##############################################################################\n",
    "class CrystalGraphConvNetWithHooks(nn.Module):\n",
    "    \"\"\"\n",
    "    - embedding: [orig_atom_fea_len -> atom_fea_len]\n",
    "    - n_conv 层 ModifiedConvLayer\n",
    "    - global mean pooling\n",
    "    - conv_to_fc -> fc_out\n",
    "    - 可在 forward 里自动 register hook\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        orig_atom_fea_len,\n",
    "        atom_fea_len,\n",
    "        nbr_fea_len,\n",
    "        n_conv,\n",
    "        h_fea_len,\n",
    "        out_dim=1,\n",
    "        classification=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.intermediate_outputs = {}\n",
    "\n",
    "        # 1) embedding\n",
    "        self.embedding = nn.Linear(orig_atom_fea_len, atom_fea_len)\n",
    "\n",
    "        # 2) n_conv layers\n",
    "        self.convs = nn.ModuleList([\n",
    "            ModifiedConvLayer(atom_fea_len, nbr_fea_len) for _ in range(n_conv)\n",
    "        ])\n",
    "\n",
    "        # 3) pooling\n",
    "        self.pooling = self._pooling_mean\n",
    "\n",
    "        # 4) conv_to_fc\n",
    "        self.conv_to_fc = nn.Linear(atom_fea_len, h_fea_len)\n",
    "        self.conv_to_fc_softplus = nn.Softplus()\n",
    "\n",
    "        # 5) fc_out\n",
    "        if classification and out_dim > 1:\n",
    "            self.fc_out = nn.Linear(h_fea_len, out_dim)\n",
    "        else:\n",
    "            self.fc_out = nn.Linear(h_fea_len, out_dim)\n",
    "\n",
    "    def _pooling_mean(self, atom_fea, crystal_atom_idx_list):\n",
    "        pooled = []\n",
    "        for idx in crystal_atom_idx_list:\n",
    "            chunk = atom_fea[idx]\n",
    "            pooled.append(chunk.mean(dim=0, keepdim=True))\n",
    "        return torch.cat(pooled, dim=0)\n",
    "\n",
    "    def add_hook(self, name):\n",
    "        def hook(module, input, output):\n",
    "            self.intermediate_outputs[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    def forward(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        # 注册 hook\n",
    "        self.embedding.register_forward_hook(self.add_hook(\"embedding\"))\n",
    "        for i, conv_layer in enumerate(self.convs):\n",
    "            conv_layer.register_forward_hook(self.add_hook(f\"conv_{i}\"))\n",
    "        self.fc_out.register_forward_hook(self.add_hook(\"fc_out\"))\n",
    "\n",
    "        # 1) embedding\n",
    "        atom_fea = self.embedding(atom_fea)\n",
    "        # 2) conv\n",
    "        for conv_layer in self.convs:\n",
    "            atom_fea = conv_layer(atom_fea, nbr_fea, nbr_fea_idx)\n",
    "\n",
    "        # 3) pooling\n",
    "        if isinstance(crystal_atom_idx, torch.Tensor):\n",
    "            crystal_atom_idx = [crystal_atom_idx]\n",
    "        crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n",
    "\n",
    "        # 4) conv_to_fc\n",
    "        crys_fea = self.conv_to_fc_softplus(self.conv_to_fc(crys_fea))\n",
    "\n",
    "        # 5) fc_out\n",
    "        out = self.fc_out(crys_fea)\n",
    "        return out\n",
    "\n",
    "    def get_graph_embedding(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx):\n",
    "        \"\"\"\n",
    "        只取 fc_out 之前的图 embedding，用于 SHAP 或可视化\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            atom_fea = self.embedding(atom_fea)\n",
    "            for conv_layer in self.convs:\n",
    "                atom_fea = conv_layer(atom_fea, nbr_fea, nbr_fea_idx)\n",
    "            if isinstance(crystal_atom_idx, torch.Tensor):\n",
    "                crystal_atom_idx = [crystal_atom_idx]\n",
    "            crys_fea = self.pooling(atom_fea, crystal_atom_idx)\n",
    "            crys_fea = self.conv_to_fc_softplus(self.conv_to_fc(crys_fea))\n",
    "        return crys_fea\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# D. 自动解析维度 + 构造模型的 ModelWrapper\n",
    "##############################################################################\n",
    "class ModelWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    用于 SHAP 阶段：自动解析 checkpoint 得到网络维度,\n",
    "    在 forward 时只做 get_graph_embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, checkpoint_path):\n",
    "        super().__init__()\n",
    "        # 1) 加载 checkpoint\n",
    "        ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "        state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt\n",
    "\n",
    "        # 2) 自动推断网络维度\n",
    "        (orig_atom_fea_len,\n",
    "         atom_fea_len,\n",
    "         nbr_fea_len,\n",
    "         n_conv,\n",
    "         h_fea_len,\n",
    "         out_dim) = infer_cgcnn_dims_from_sd(state_dict)\n",
    "\n",
    "        print(\"[ModelWrapper] Inferred dims =>\",\n",
    "              f\"orig_atom_fea_len={orig_atom_fea_len},\",\n",
    "              f\"atom_fea_len={atom_fea_len},\",\n",
    "              f\"nbr_fea_len={nbr_fea_len},\",\n",
    "              f\"n_conv={n_conv},\",\n",
    "              f\"h_fea_len={h_fea_len},\",\n",
    "              f\"out_dim={out_dim}\"\n",
    "        )\n",
    "\n",
    "        # 3) 构造网络\n",
    "        classification = (out_dim > 1)\n",
    "        self.model = CrystalGraphConvNetWithHooks(\n",
    "            orig_atom_fea_len=orig_atom_fea_len,\n",
    "            atom_fea_len=atom_fea_len,\n",
    "            nbr_fea_len=nbr_fea_len,\n",
    "            n_conv=n_conv,\n",
    "            h_fea_len=h_fea_len,\n",
    "            out_dim=out_dim,\n",
    "            classification=classification\n",
    "        )\n",
    "\n",
    "        # 4) 加载权重\n",
    "        self.model.load_state_dict(state_dict, strict=True)\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, graph_batch):\n",
    "        # 只做 get_graph_embedding\n",
    "        atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx = graph_batch\n",
    "        return self.model.get_graph_embedding(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# E. Layer 可视化工具\n",
    "##############################################################################\n",
    "def visualize_layer_output(layer_name, output):\n",
    "    arr = output.squeeze().cpu().numpy()\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    if arr.ndim == 0:\n",
    "        plt.text(0.5, 0.5, f\"{layer_name} scalar: {arr:.4f}\",\n",
    "                 ha='center', va='center', fontsize=14)\n",
    "        plt.axis('off')\n",
    "    elif arr.ndim == 1:\n",
    "        plt.plot(arr, marker='o')\n",
    "        plt.title(f\"{layer_name} (1D)\")\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Value')\n",
    "    elif arr.ndim == 2:\n",
    "        plt.imshow(arr, aspect='auto', cmap='viridis')\n",
    "        plt.colorbar()\n",
    "        plt.title(f\"{layer_name} (2D)\")\n",
    "        plt.xlabel('Feature dim')\n",
    "        plt.ylabel('Sample index')\n",
    "    else:\n",
    "        print(f\"[visualize_layer_output] shape {arr.shape} not easily visualizable.\")\n",
    "        plt.close()\n",
    "        return\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{layer_name}_output.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def analyze_feature_activation(layer_name, output):\n",
    "    arr = output.squeeze().cpu().numpy()\n",
    "    if arr.ndim != 2:\n",
    "        print(f\"[analyze_feature_activation] skip, arr.ndim={arr.ndim}\")\n",
    "        return\n",
    "\n",
    "    mean_act = np.mean(arr, axis=0)\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(range(len(mean_act)), mean_act)\n",
    "    plt.title(f\"{layer_name} Mean Activation\")\n",
    "    plt.xlabel('Channel index')\n",
    "    plt.ylabel('Mean')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{layer_name}_mean_activation.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# F. dissect_single_sample: 可视化单样本卷积层\n",
    "##############################################################################\n",
    "def dissect_single_sample(model_ckpt_path, data_path):\n",
    "    print(\"\\n=== [dissect_single_sample] ===\")\n",
    "    # 1) 同样先自动解析checkpoint, 构建网络\n",
    "    #    但在这里, 我们要用 \"CrystalGraphConvNetWithHooks\" 直接 forward\n",
    "    #    (因为要看中间层)\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt\n",
    "\n",
    "    (orig_atom_fea_len,\n",
    "     atom_fea_len,\n",
    "     nbr_fea_len,\n",
    "     n_conv,\n",
    "     h_fea_len,\n",
    "     out_dim) = infer_cgcnn_dims_from_sd(state_dict)\n",
    "\n",
    "    classification = (out_dim > 1)\n",
    "    model = CrystalGraphConvNetWithHooks(\n",
    "        orig_atom_fea_len=orig_atom_fea_len,\n",
    "        atom_fea_len=atom_fea_len,\n",
    "        nbr_fea_len=nbr_fea_len,\n",
    "        n_conv=n_conv,\n",
    "        h_fea_len=h_fea_len,\n",
    "        out_dim=out_dim,\n",
    "        classification=classification\n",
    "    )\n",
    "    model.load_state_dict(state_dict, strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    print(\"[dissect_single_sample] =>\",\n",
    "          f\"orig_atom_fea_len={orig_atom_fea_len}, atom_fea_len={atom_fea_len},\",\n",
    "          f\"nbr_fea_len={nbr_fea_len}, n_conv={n_conv}, h_fea_len={h_fea_len}, out_dim={out_dim}\")\n",
    "\n",
    "    # 2) 从 dataset 取 1 个样本\n",
    "    #    注意, 要和训练同样的 radius/其他参数\n",
    "    dataset = CIFData(data_path, radius=20.0)\n",
    "    loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=collate_pool)\n",
    "    sample_input = next(iter(loader))\n",
    "    graph_inputs, target, cif_id = sample_input\n",
    "\n",
    "    # 3) 如果只有3个,自己做 idx\n",
    "    if len(graph_inputs) == 3:\n",
    "        atom_fea, nbr_fea, crystal_atom_idx = graph_inputs\n",
    "        nbr_fea_idx = torch.arange(atom_fea.shape[0]).unsqueeze(-1)\n",
    "        graph_inputs = (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "\n",
    "    # 4) forward\n",
    "    with torch.no_grad():\n",
    "        output = model(*graph_inputs)\n",
    "    print(\"[dissect_single_sample] forward done. Output shape:\", output.shape)\n",
    "    if output.numel() == 1:\n",
    "        print(\"Model output value:\", output.item())\n",
    "    else:\n",
    "        print(\"Model output (batch):\", output)\n",
    "\n",
    "    # 5) 可视化中间层\n",
    "    print(\"\\n[dissect_single_sample] Intermediate outputs:\")\n",
    "    for name, out_tensor in model.intermediate_outputs.items():\n",
    "        print(f\"  {name}: shape={tuple(out_tensor.shape)}\")\n",
    "        visualize_layer_output(name, out_tensor)\n",
    "        analyze_feature_activation(name, out_tensor)\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# G. run_shap_analysis: 做图级嵌入 + SHAP\n",
    "##############################################################################\n",
    "def run_shap_analysis(model_ckpt_path, data_path):\n",
    "    print(\"\\n=== [run_shap_analysis] ===\")\n",
    "    # 直接用自动推断的 ModelWrapper\n",
    "    emb_model = ModelWrapper(model_ckpt_path)\n",
    "    emb_model.eval()\n",
    "\n",
    "    # dataset\n",
    "    dataset = CIFData(data_path, radius=20.0)\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False, collate_fn=collate_pool)\n",
    "\n",
    "    X_emb_list, y_list = [], []\n",
    "    for batch in loader:\n",
    "        graph_inputs, targets, cif_ids = batch\n",
    "        if len(graph_inputs) == 3:\n",
    "            atom_fea, nbr_fea, crystal_atom_idx = graph_inputs\n",
    "            nbr_fea_idx = torch.arange(atom_fea.shape[0]).unsqueeze(-1)\n",
    "            graph_inputs = (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        emb = emb_model(graph_inputs).cpu()\n",
    "        X_emb_list.append(emb)\n",
    "        y_list.append(targets)\n",
    "\n",
    "    X_emb = torch.cat(X_emb_list, dim=0)\n",
    "    y_test = torch.cat(y_list).numpy()\n",
    "    print(\"Graph embedding shape:\", X_emb.shape)\n",
    "    print(\"y_test shape:\", y_test.shape)\n",
    "\n",
    "    # ---------------------------\n",
    "    # 下面是你原有的 fc_out & predict\n",
    "    # ---------------------------\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    state_dict = ckpt['state_dict'] if 'state_dict' in ckpt else ckpt\n",
    "    fc_w = state_dict['fc_out.weight'].cpu()\n",
    "    fc_b = state_dict['fc_out.bias'].cpu()\n",
    "\n",
    "    def predict_from_emb(X_numpy):\n",
    "        X_t = torch.tensor(X_numpy, dtype=torch.float32)\n",
    "        out = X_t @ fc_w.T + fc_b\n",
    "        return out.squeeze(-1).detach().numpy()\n",
    "\n",
    "    # shap\n",
    "    background_size = min(200, X_emb.shape[0])\n",
    "    background_data = X_emb[:background_size].numpy()\n",
    "\n",
    "    explainer = shap.KernelExplainer(predict_from_emb, background_data)\n",
    "    sample_size = min(200, X_emb.shape[0])\n",
    "    sample_data = X_emb[:sample_size].numpy()\n",
    "\n",
    "    shap_values = explainer.shap_values(sample_data)\n",
    "    if isinstance(shap_values, list):\n",
    "        shap_values = shap_values[0]\n",
    "    shap_values = np.array(shap_values).reshape(sample_size, -1)\n",
    "    print(\"SHAP values shape:\", shap_values.shape)\n",
    "\n",
    "    emb_dim = shap_values.shape[1]\n",
    "    feature_names = [f\"GNN_Emb_{i}\" for i in range(emb_dim)]\n",
    "\n",
    "    # summary plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values, sample_data, feature_names=feature_names,\n",
    "                      plot_type='dot', max_display=10, show=False)\n",
    "    plt.savefig('shap_summary_swarm.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"[SHAP] summary swarm plot saved -> shap_summary_swarm.png\")\n",
    "\n",
    "    # top K\n",
    "    mean_importance = np.mean(np.abs(shap_values), axis=0)\n",
    "    sorted_idx = np.argsort(mean_importance)\n",
    "    top_k = 10\n",
    "    print(f\"\\nTop {top_k} important embedding dims:\")\n",
    "    for i in range(1, top_k + 1):\n",
    "        idx = sorted_idx[-i]\n",
    "        print(f\"  {feature_names[idx]} = {mean_importance[idx]:.4f}\")\n",
    "\n",
    "    # save shap\n",
    "    df_shap = pd.DataFrame(shap_values, columns=feature_names)\n",
    "    df_shap.to_csv('shap_values_emb.csv', index=False)\n",
    "    print(\"[SHAP] saved shap_values_emb.csv\")\n",
    "\n",
    "    df_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': mean_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    df_importance.to_csv('feature_importance_emb.csv', index=False)\n",
    "    print(\"[SHAP] saved feature_importance_emb.csv\")\n",
    "\n",
    "    # ==================================================\n",
    "    # 在这里，把与 shap_values 行数对应的 embedding 也保存\n",
    "    # ==================================================\n",
    "    df_emb_for_shap = pd.DataFrame(sample_data, columns=feature_names)\n",
    "    # 如果想带上对应的目标值\n",
    "    df_emb_for_shap[\"target\"] = y_test[:sample_size]\n",
    "    df_emb_for_shap.to_csv(\"shap_input_emb.csv\", index=False)\n",
    "    print(\"[SHAP] saved shap_input_emb.csv (the embedding input to shap).\")\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# H. main\n",
    "##############################################################################\n",
    "def main():\n",
    "    model_ckpt_path = r\"model_best.pth.tar\"\n",
    "    data_path = r\"E:\\桌面文件\\香港科技大学固态电解质项目\\project2_1\\文章整理_project2\\CGCNN\\cgcnn-master_2\\data\\sample-regression\\dielectricity\"\n",
    "\n",
    "    # 1) 对单样本做卷积层可视化\n",
    "    dissect_single_sample(model_ckpt_path, data_path)\n",
    "\n",
    "    # 2) 对全数据做 SHAP\n",
    "    run_shap_analysis(model_ckpt_path, data_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
