{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\cgcnn\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\conda\\envs\\cgcnn\\Lib\\site-packages\\pymatgen\\io\\cif.py:1224: UserWarning: Issues encountered while parsing CIF: 1 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  warnings.warn(\"Issues encountered while parsing CIF: \" + \"\\n\".join(self.warnings))\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C5FH4NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C5Cl2F2HNO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C3F3H4NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C2ClF2H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C4FH3N2O not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C2F3H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C3F5H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([26087, 92])\n",
      "y_test shape: (711, 1)\n",
      "Shape of X_test[:100]: torch.Size([100, 92])\n",
      "Input data shape: torch.Size([1, 100, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<01:27,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:01<01:26,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:02<01:27,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:03<01:26,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:04<01:26,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:05<01:26,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:06<01:25,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:07<01:25,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:08<01:23,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:09<01:23,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:10<01:22,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:11<01:22,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:11<01:21,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:12<01:19,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:13<01:19,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:14<01:17,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:15<01:16,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:16<01:14,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:17<01:14,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:18<01:14,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:19<01:13,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:20<01:13,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:21<01:12,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:22<01:11,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:23<01:10,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:24<01:10,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:25<01:11,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:26<01:10,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:27<01:08,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:28<01:06,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:28<01:05,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:29<01:04,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:30<01:02,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:31<01:01,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:32<00:59,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:33<00:59,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:34<00:58,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:35<00:57,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:36<00:57,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:37<00:55,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:38<00:55,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:39<00:57,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:40<00:56,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:41<00:55,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:42<00:53,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:43<00:53,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:44<00:51,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:45<00:49,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:46<00:48,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:47<00:48,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:48<00:48,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:49<00:48,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:50<00:48,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:51<00:48,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:52<00:47,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:53<00:44,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:54<00:44,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:55<00:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:56<00:43,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:57<00:42,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:58<00:41,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:59<00:39,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [01:00<00:37,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [01:01<00:35,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [01:02<00:34,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [01:03<00:32,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [01:04<00:31,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [01:05<00:30,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [01:06<00:29,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [01:07<00:28,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [01:08<00:27,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [01:09<00:26,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [01:10<00:25,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [01:10<00:24,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [01:11<00:23,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [01:12<00:22,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [01:13<00:21,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [01:14<00:20,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [01:15<00:19,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:16<00:18,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [01:17<00:18,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [01:18<00:16,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [01:19<00:16,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [01:20<00:14,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [01:21<00:14,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [01:22<00:13,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [01:23<00:12,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [01:24<00:11,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [01:25<00:10,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [01:26<00:09,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [01:27<00:08,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [01:27<00:07,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [01:28<00:06,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [01:29<00:05,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [01:30<00:04,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [01:31<00:03,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [01:32<00:02,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [01:33<00:01,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [01:34<00:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1150x660 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP analysis completed. Results saved in 'shap_summary.png'.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "from cgcnn.data import CIFData\n",
    "from cgcnn.data import collate_pool\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据\n",
    "dataset = CIFData('data/sample-regression/dielectricity')\n",
    "test_loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=0,\n",
    "                         collate_fn=collate_pool)\n",
    "\n",
    "# 加载模型\n",
    "best_model = torch.load('model_best.pth.tar', map_location=torch.device('cpu'))\n",
    "\n",
    "# 创建一个包装器类\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, state_dict):\n",
    "        super().__init__()\n",
    "        self.state_dict = {k: v.cpu() for k, v in state_dict.items()}\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        input_data = args[0]  # 获取输入数据\n",
    "        \n",
    "        # 调试输出\n",
    "        print(f\"Input data shape: {input_data.shape}\")\n",
    "        print(f\"fc_out.weight shape: {self.state_dict['fc_out.weight'].shape}\")\n",
    "        print(f\"fc_out.bias shape: {self.state_dict['fc_out.bias'].shape}\")\n",
    "        \n",
    "        # 确保输入是2D的\n",
    "        if input_data.dim() > 2:\n",
    "            input_data = input_data.view(-1, input_data.size(-1))\n",
    "        \n",
    "        # 如果输入特征数不匹配，我们需要调整输入\n",
    "        if input_data.shape[-1] < self.state_dict['fc_out.weight'].shape[1]:\n",
    "            print(f\"Padding input feature size from {input_data.shape[-1]} to {self.state_dict['fc_out.weight'].shape[1]}\")\n",
    "            padding = torch.zeros(input_data.size(0), self.state_dict['fc_out.weight'].shape[1] - input_data.size(1))\n",
    "            input_data = torch.cat([input_data, padding], dim=1)\n",
    "        elif input_data.shape[-1] > self.state_dict['fc_out.weight'].shape[1]:\n",
    "            print(f\"Truncating input feature size from {input_data.shape[-1]} to {self.state_dict['fc_out.weight'].shape[1]}\")\n",
    "            input_data = input_data[:, :self.state_dict['fc_out.weight'].shape[1]]\n",
    "        \n",
    "        # 矩阵乘法\n",
    "        output = torch.matmul(input_data, self.state_dict['fc_out.weight'].t()) + self.state_dict['fc_out.bias']\n",
    "        return output\n",
    "\n",
    "# 使用包装器类创建模型\n",
    "model = ModelWrapper(best_model['state_dict'])\n",
    "\n",
    "# 准备数据\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i, (input, target, _) in enumerate(test_loader):\n",
    "    X_test.append(input[0].cpu())\n",
    "    y_test.append(target.cpu())\n",
    "\n",
    "X_test = torch.cat(X_test, dim=0)\n",
    "y_test = torch.cat(y_test).numpy()\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# 定义一个包装函数来适配SHAP\n",
    "def f(X):\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        if X.dim() == 2:\n",
    "            X = X.unsqueeze(0)  # 添加批次维度\n",
    "        output = model(X)\n",
    "    return output.numpy()\n",
    "\n",
    "# 创建SHAP解释器\n",
    "print(\"Shape of X_test[:100]:\", X_test[:100].shape)\n",
    "explainer = shap.KernelExplainer(f, X_test[:100].numpy())\n",
    "\n",
    "# 计算SHAP值（使用一小部分数据以加快计算）\n",
    "sample_size = min(100, len(X_test))\n",
    "sample_data = X_test[:sample_size].numpy()\n",
    "shap_values = explainer.shap_values(sample_data)\n",
    "\n",
    "# 可视化SHAP值\n",
    "plt.figure(figsize=(10, 6))\n",
    "shap.summary_plot(shap_values, sample_data, plot_type=\"bar\")\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"SHAP analysis completed. Results saved in 'shap_summary.png'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelWrapper initialized with keys: dict_keys(['embedding.weight', 'embedding.bias', 'convs.0.fc_full.weight', 'convs.0.fc_full.bias', 'convs.0.bn1.weight', 'convs.0.bn1.bias', 'convs.0.bn1.running_mean', 'convs.0.bn1.running_var', 'convs.0.bn1.num_batches_tracked', 'convs.0.bn2.weight', 'convs.0.bn2.bias', 'convs.0.bn2.running_mean', 'convs.0.bn2.running_var', 'convs.0.bn2.num_batches_tracked', 'convs.1.fc_full.weight', 'convs.1.fc_full.bias', 'convs.1.bn1.weight', 'convs.1.bn1.bias', 'convs.1.bn1.running_mean', 'convs.1.bn1.running_var', 'convs.1.bn1.num_batches_tracked', 'convs.1.bn2.weight', 'convs.1.bn2.bias', 'convs.1.bn2.running_mean', 'convs.1.bn2.running_var', 'convs.1.bn2.num_batches_tracked', 'convs.2.fc_full.weight', 'convs.2.fc_full.bias', 'convs.2.bn1.weight', 'convs.2.bn1.bias', 'convs.2.bn1.running_mean', 'convs.2.bn1.running_var', 'convs.2.bn1.num_batches_tracked', 'convs.2.bn2.weight', 'convs.2.bn2.bias', 'convs.2.bn2.running_mean', 'convs.2.bn2.running_var', 'convs.2.bn2.num_batches_tracked', 'conv_to_fc.weight', 'conv_to_fc.bias', 'fc_out.weight', 'fc_out.bias'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\cgcnn\\Lib\\site-packages\\pymatgen\\io\\cif.py:1224: UserWarning: Issues encountered while parsing CIF: 1 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  warnings.warn(\"Issues encountered while parsing CIF: \" + \"\\n\".join(self.warnings))\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C5FH4NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C5Cl2F2HNO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C3F3H4NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C2ClF2H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C4FH3N2O not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C2F3H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C3F5H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: torch.Size([26087, 92])\n",
      "y_test shape: (711, 1)\n",
      "X_test first few rows:\n",
      "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0.]])\n",
      "Shape of X_test[:100]: torch.Size([100, 92])\n",
      "Input data shape: torch.Size([1, 100, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([100, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:00<01:30,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:01<01:25,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [00:02<01:23,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [00:03<01:22,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [00:04<01:21,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [00:05<01:19,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [00:06<01:20,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [00:06<01:18,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [00:07<01:17,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:08<01:17,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [00:09<01:16,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:10<01:14,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [00:11<01:14,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [00:11<01:13,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [00:12<01:12,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [00:13<01:11,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [00:14<01:11,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 18/100 [00:15<01:11,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 19/100 [00:16<01:10,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:17<01:09,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 21/100 [00:18<01:08,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:18<01:08,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 23/100 [00:19<01:07,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 24/100 [00:20<01:06,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 25/100 [00:21<01:05,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 26/100 [00:22<01:05,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 27/100 [00:23<01:04,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 28/100 [00:24<01:03,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 29/100 [00:25<01:03,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:26<01:02,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 31/100 [00:26<01:01,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:27<00:59,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 33/100 [00:28<00:59,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 34/100 [00:29<00:57,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 35/100 [00:30<00:57,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 36/100 [00:31<00:55,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 37/100 [00:32<00:54,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 38/100 [00:32<00:53,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 39/100 [00:33<00:52,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:34<00:51,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 41/100 [00:35<00:50,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:36<00:49,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:37<00:48,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 44/100 [00:38<00:47,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 45/100 [00:38<00:46,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 46/100 [00:39<00:46,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 47/100 [00:40<00:45,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 48/100 [00:41<00:44,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 49/100 [00:42<00:43,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:43<00:42,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 51/100 [00:44<00:41,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:44<00:40,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 53/100 [00:45<00:39,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 54/100 [00:46<00:38,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 55/100 [00:47<00:38,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 56/100 [00:48<00:37,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:49<00:36,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 58/100 [00:50<00:36,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 59/100 [00:50<00:34,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [00:51<00:33,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 61/100 [00:52<00:33,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:53<00:32,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63/100 [00:54<00:31,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:55<00:30,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 65/100 [00:55<00:29,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 66/100 [00:56<00:28,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 67/100 [00:57<00:28,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 68/100 [00:58<00:27,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 69/100 [00:59<00:26,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [01:00<00:25,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 71/100 [01:01<00:24,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [01:01<00:23,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 73/100 [01:02<00:23,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 74/100 [01:03<00:22,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 75/100 [01:04<00:21,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 76/100 [01:05<00:20,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 77/100 [01:06<00:20,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 78/100 [01:07<00:19,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 79/100 [01:08<00:18,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:09<00:18,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 81/100 [01:09<00:17,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 82/100 [01:10<00:15,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 83/100 [01:11<00:14,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 84/100 [01:12<00:14,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 85/100 [01:13<00:13,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 86/100 [01:14<00:12,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 87/100 [01:15<00:11,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 88/100 [01:16<00:10,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 89/100 [01:17<00:09,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [01:17<00:08,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 91/100 [01:18<00:07,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 92/100 [01:19<00:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 93/100 [01:20<00:06,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 94/100 [01:21<00:05,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 95/100 [01:22<00:04,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 96/100 [01:23<00:03,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 97/100 [01:23<00:02,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [01:24<00:01,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [01:25<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data shape: torch.Size([1, 1, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([1, 1])\n",
      "Input data shape: torch.Size([1, 211600, 92])\n",
      "fc_out.weight shape: torch.Size([1, 128])\n",
      "fc_out.bias shape: torch.Size([1])\n",
      "Padding input feature size from 92 to 128\n",
      "Output shape: torch.Size([211600, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:26<00:00,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values shape: (100, 92, 1)\n",
      "Reshaped SHAP values shape: (100, 92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x950 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP analysis completed. Results saved in 'shap_summary.png'.\n",
      "\n",
      "Top 10 most important features:\n",
      "Feature 49: 0.1492\n",
      "Feature 73: 0.0836\n",
      "Feature 78: 0.0612\n",
      "Feature 46: 0.0521\n",
      "Feature 74: 0.0501\n",
      "Feature 1: 0.0388\n",
      "Feature 30: 0.0381\n",
      "Feature 36: 0.0194\n",
      "Feature 31: 0.0188\n",
      "Feature 14: 0.0175\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "from cgcnn.data import CIFData\n",
    "from cgcnn.data import collate_pool\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据\n",
    "dataset = CIFData('data/sample-regression/dielectricity')\n",
    "test_loader = DataLoader(dataset, batch_size=256, shuffle=False, num_workers=0,\n",
    "                         collate_fn=collate_pool)\n",
    "\n",
    "# 加载模型\n",
    "best_model = torch.load('model_best.pth.tar', map_location=torch.device('cpu'))\n",
    "\n",
    "# 创建一个包装器类\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, state_dict):\n",
    "        super().__init__()\n",
    "        self.state_dict = {k: v.cpu() for k, v in state_dict.items()}\n",
    "        print(\"ModelWrapper initialized with keys:\", self.state_dict.keys())\n",
    "\n",
    "    def forward(self, *args):\n",
    "        input_data = args[0]  # 获取输入数据\n",
    "        \n",
    "        # 调试输出\n",
    "        print(f\"Input data shape: {input_data.shape}\")\n",
    "        print(f\"fc_out.weight shape: {self.state_dict['fc_out.weight'].shape}\")\n",
    "        print(f\"fc_out.bias shape: {self.state_dict['fc_out.bias'].shape}\")\n",
    "        \n",
    "        # 确保输入是2D的\n",
    "        if input_data.dim() > 2:\n",
    "            input_data = input_data.view(-1, input_data.size(-1))\n",
    "        \n",
    "        # 如果输入特征数不匹配，我们需要调整输入\n",
    "        if input_data.shape[-1] < self.state_dict['fc_out.weight'].shape[1]:\n",
    "            print(f\"Padding input feature size from {input_data.shape[-1]} to {self.state_dict['fc_out.weight'].shape[1]}\")\n",
    "            padding = torch.zeros(input_data.size(0), self.state_dict['fc_out.weight'].shape[1] - input_data.size(1))\n",
    "            input_data = torch.cat([input_data, padding], dim=1)\n",
    "        elif input_data.shape[-1] > self.state_dict['fc_out.weight'].shape[1]:\n",
    "            print(f\"Truncating input feature size from {input_data.shape[-1]} to {self.state_dict['fc_out.weight'].shape[1]}\")\n",
    "            input_data = input_data[:, :self.state_dict['fc_out.weight'].shape[1]]\n",
    "        \n",
    "        # 矩阵乘法\n",
    "        output = torch.matmul(input_data, self.state_dict['fc_out.weight'].t()) + self.state_dict['fc_out.bias']\n",
    "        print(f\"Output shape: {output.shape}\")\n",
    "        return output\n",
    "\n",
    "# 使用包装器类创建模型\n",
    "model = ModelWrapper(best_model['state_dict'])\n",
    "\n",
    "# 准备数据\n",
    "X_test = []\n",
    "y_test = []\n",
    "for i, (input, target, _) in enumerate(test_loader):\n",
    "    X_test.append(input[0].cpu())\n",
    "    y_test.append(target.cpu())\n",
    "\n",
    "X_test = torch.cat(X_test, dim=0)\n",
    "y_test = torch.cat(y_test).numpy()\n",
    "\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "print(\"X_test first few rows:\")\n",
    "print(X_test[:5])\n",
    "\n",
    "# 定义一个包装函数来适配SHAP\n",
    "def f(X):\n",
    "    with torch.no_grad():\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        if X.dim() == 2:\n",
    "            X = X.unsqueeze(0)  # 添加批次维度\n",
    "        output = model(X)\n",
    "    return output.numpy()\n",
    "\n",
    "# 创建SHAP解释器\n",
    "print(\"Shape of X_test[:100]:\", X_test[:100].shape)\n",
    "explainer = shap.KernelExplainer(f, X_test[:100].numpy())\n",
    "\n",
    "# 计算SHAP值\n",
    "sample_size = min(100, len(X_test))\n",
    "sample_data = X_test[:sample_size].numpy()\n",
    "shap_values = explainer.shap_values(sample_data)\n",
    "\n",
    "print(\"SHAP values shape:\", np.array(shap_values).shape)\n",
    "\n",
    "# 重塑SHAP值\n",
    "shap_values_reshaped = np.array(shap_values).reshape(sample_size, -1)\n",
    "print(\"Reshaped SHAP values shape:\", shap_values_reshaped.shape)\n",
    "\n",
    "# 可视化SHAP值\n",
    "plt.figure(figsize=(15, 10))\n",
    "shap.summary_plot(shap_values_reshaped, sample_data, plot_type=\"bar\", max_display=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"SHAP analysis completed. Results saved in 'shap_summary.png'.\")\n",
    "\n",
    "# 打印前10个特征的重要性\n",
    "feature_importance = np.abs(shap_values_reshaped).mean(0)\n",
    "sorted_idx = feature_importance.argsort()\n",
    "print(\"\\nTop 10 most important features:\")\n",
    "for i in range(1, 11):\n",
    "    print(f\"Feature {sorted_idx[-i]}: {feature_importance[sorted_idx[-i]]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 711\n",
      "\n",
      "Sample structure:\n",
      "Sample type: <class 'tuple'>, length: 3\n",
      "\n",
      "Item 0:\n",
      "  Type: <class 'tuple'>\n",
      "  Length: 3\n",
      "    Sub-item 0: Tensor with shape torch.Size([32, 92]) and dtype torch.float32\n",
      "    Sub-item 1: Tensor with shape torch.Size([32, 12, 41]) and dtype torch.float32\n",
      "    Sub-item 2: Tensor with shape torch.Size([32, 12]) and dtype torch.int64\n",
      "\n",
      "Item 1:\n",
      "  Type: <class 'torch.Tensor'>\n",
      "  Shape: torch.Size([1])\n",
      "  dtype: torch.float32\n",
      "  Value: 2.3264262676239014\n",
      "\n",
      "Item 2:\n",
      "  Type: <class 'str'>\n",
      "  Value: C9Cl4F8H7NO3\n",
      "\n",
      "Sample id_prop_data:\n",
      "  C9Cl4F8H7NO3: 2.326426303\n",
      "  C29ClFH35N3O6: 2.798037066\n",
      "  C11FH10N3O2: 2.654976366\n",
      "  C17FH14NO3: 2.760385026\n",
      "  C17F6H14N2O3: 2.506814594\n",
      "\n",
      "Note: The exact meaning of features may require referring to the CGCNN implementation or documentation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from cgcnn.data import CIFData\n",
    "from cgcnn.data import collate_pool\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 加载数据\n",
    "dataset = CIFData('data/sample-regression/dielectricity')\n",
    "\n",
    "print(\"Dataset length:\", len(dataset))\n",
    "\n",
    "# 获取一个样本数据\n",
    "sample = dataset[0]\n",
    "print(\"\\nSample structure:\")\n",
    "print(f\"Sample type: {type(sample)}, length: {len(sample)}\")\n",
    "\n",
    "# 详细检查样本的每个元素\n",
    "for i, item in enumerate(sample):\n",
    "    print(f\"\\nItem {i}:\")\n",
    "    print(f\"  Type: {type(item)}\")\n",
    "    if isinstance(item, tuple):\n",
    "        print(f\"  Length: {len(item)}\")\n",
    "        for j, sub_item in enumerate(item):\n",
    "            if isinstance(sub_item, torch.Tensor):\n",
    "                print(f\"    Sub-item {j}: Tensor with shape {sub_item.shape} and dtype {sub_item.dtype}\")\n",
    "            else:\n",
    "                print(f\"    Sub-item {j}: {type(sub_item)}\")\n",
    "    elif isinstance(item, torch.Tensor):\n",
    "        print(f\"  Shape: {item.shape}\")\n",
    "        print(f\"  dtype: {item.dtype}\")\n",
    "        print(f\"  Value: {item.item()}\")\n",
    "    elif isinstance(item, str):\n",
    "        print(f\"  Value: {item}\")\n",
    "\n",
    "# 打印一些 id_prop_data 的示例\n",
    "print(\"\\nSample id_prop_data:\")\n",
    "for i, (id, prop) in enumerate(dataset.id_prop_data[:5]):\n",
    "    print(f\"  {id}: {prop}\")\n",
    "\n",
    "print(\"\\nNote: The exact meaning of features may require referring to the CGCNN implementation or documentation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\conda\\envs\\cgcnn\\Lib\\site-packages\\pymatgen\\io\\cif.py:1224: UserWarning: Issues encountered while parsing CIF: 1 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  warnings.warn(\"Issues encountered while parsing CIF: \" + \"\\n\".join(self.warnings))\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C5FH4NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C5Cl2F2HNO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C3F3H4NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C2ClF2H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C4FH3N2O not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C2F3H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n",
      "c:\\Users\\ASUS\\Desktop\\文章整理_project2\\CGCNN\\cgcnn-master\\cgcnn\\data.py:333: UserWarning: C3F5H2NO not find enough neighbors to build graph. If it happens frequently, consider increase radius.\n",
      "  warnings.warn('{} not find enough neighbors to build graph. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations saved as 'feature_visualization.png' and 'top_features_analysis.png'\n"
     ]
    }
   ],
   "source": [
    "###################数据降维PCA/t-SNE，特征可视化###############\n",
    "import torch\n",
    "from cgcnn.data import CIFData\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 加载数据\n",
    "dataset = CIFData('data/sample-regression/dielectricity')\n",
    "\n",
    "# 提取特征和标签\n",
    "features = []\n",
    "labels = []\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    atom_features = sample[0][0]  # 假设这是原子特征\n",
    "    features.append(atom_features.mean(dim=0).numpy())  # 取平均值作为分子特征\n",
    "    labels.append(sample[1].item())\n",
    "\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# 应用 PCA\n",
    "pca = PCA(n_components=2)\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# 应用 t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "features_tsne = tsne.fit_transform(features)\n",
    "\n",
    "# 可视化 PCA 结果\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "scatter = plt.scatter(features_pca[:, 0], features_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.colorbar(scatter, label='Dielectric Constant')\n",
    "plt.title('PCA Visualization')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "\n",
    "# 可视化 t-SNE 结果\n",
    "plt.subplot(122)\n",
    "scatter = plt.scatter(features_tsne[:, 0], features_tsne[:, 1], c=labels, cmap='viridis')\n",
    "plt.colorbar(scatter, label='Dielectric Constant')\n",
    "plt.title('t-SNE Visualization')\n",
    "plt.xlabel('t-SNE Feature 1')\n",
    "plt.ylabel('t-SNE Feature 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_visualization.png')\n",
    "plt.close()\n",
    "\n",
    "# 分析 SHAP 重要性最高的特征\n",
    "top_features = [49, 73, 78, 46, 74]  # 根据之前的 SHAP 分析结果\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, feature in enumerate(top_features):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.scatter(features[:, feature], labels)\n",
    "    plt.title(f'Feature {feature}')\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Dielectric Constant')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('top_features_analysis.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"Visualizations saved as 'feature_visualization.png' and 'top_features_analysis.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP dependence plot for feature 49 saved.\n",
      "SHAP dependence plot for feature 73 saved.\n",
      "SHAP dependence plot for feature 78 saved.\n",
      "SHAP dependence plot for feature 46 saved.\n",
      "SHAP dependence plot for feature 74 saved.\n",
      "SHAP dependence plots completed.\n",
      "SHAP dependence plot for feature 49 saved.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制某个特征的 SHAP 值影响图\n",
    "def plot_feature_shap(feature_index, shap_values, input_data, feature_names=None):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    shap.dependence_plot(\n",
    "        feature_index, \n",
    "        shap_values_reshaped, \n",
    "        sample_data,\n",
    "        feature_names=feature_names,\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f\"SHAP Dependence Plot for Feature {feature_index}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'shap_dependence_feature_{feature_index}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# 获取特征名称（如果有的话）\n",
    "feature_names = [f\"Feature_{i}\" for i in range(sample_data.shape[1])]\n",
    "\n",
    "# 绘制前5个最重要特征的 SHAP 值影响图\n",
    "for i in range(5):\n",
    "    feature_index = sorted_idx[-(i+1)]\n",
    "    plot_feature_shap(feature_index, shap_values_reshaped, sample_data, feature_names)\n",
    "    print(f\"SHAP dependence plot for feature {feature_index} saved.\")\n",
    "\n",
    "print(\"SHAP dependence plots completed.\")\n",
    "# 绘制第49个特征的 SHAP 值影响图\n",
    "feature_index = 49\n",
    "plot_feature_shap(feature_index, shap_values_reshaped, sample_data, feature_names)\n",
    "print(f\"SHAP dependence plot for feature {feature_index} saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgcnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
